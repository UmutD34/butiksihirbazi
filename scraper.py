"""
PaÅŸabahÃ§e Butik Koleksiyonlar - Cloudflare Bypass Scraper
Playwright ile tam tarayÄ±cÄ± simÃ¼lasyonu kullanarak 312 Ã¼rÃ¼nÃ¼ tarar
"""

import asyncio
import csv
import json
from playwright.async_api import async_playwright
from datetime import datetime
import re

class PasabahceScraper:
    def __init__(self):
        self.base_url = "https://www.pasabahcemagazalari.com/butik-koleksiyonlar"
        self.products = []
        
    async def scrape_all_products(self):
        """TÃ¼m Ã¼rÃ¼nleri tarar ve CSV'ye kaydeder"""
        async with async_playwright() as p:
            # Chromium baÅŸlat (daha az ÅŸÃ¼pheli)
            browser = await p.chromium.launch(
                headless=False,  # Debug iÃ§in False, production'da True
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox'
                ]
            )
            
            # Stealth ayarlarÄ±
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='tr-TR',
                timezone_id='Europe/Istanbul'
            )
            
            # JavaScript ile automation flag'i gizle
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            page = await context.new_page()
            
            print("ğŸš€ Scraping baÅŸlatÄ±lÄ±yor...")
            
            page_num = 1
            while True:
                url = f"{self.base_url}?p={page_num}"
                print(f"\nğŸ“„ Sayfa {page_num} taranÄ±yor: {url}")
                
                try:
                    await page.goto(url, wait_until='networkidle', timeout=60000)
                    
                    # Cloudflare challenge bekle
                    await asyncio.sleep(3)
                    
                    # ÃœrÃ¼n kartlarÄ±nÄ± bul
                    products = await page.query_selector_all('.product-item')
                    
                    if not products:
                        print(f"âœ… Sayfa {page_num-1}'de tarama tamamlandÄ±!")
                        break
                    
                    print(f"   ğŸ” {len(products)} Ã¼rÃ¼n bulundu")
                    
                    for product in products:
                        try:
                            # ÃœrÃ¼n adÄ±
                            name_elem = await product.query_selector('.product-name')
                            name = await name_elem.inner_text() if name_elem else "Ä°simsiz ÃœrÃ¼n"
                            
                            # ÃœrÃ¼n linki
                            link_elem = await product.query_selector('a.product-item-link')
                            link = await link_elem.get_attribute('href') if link_elem else ""
                            
                            # ÃœrÃ¼n resmi
                            img_elem = await product.query_selector('img.product-image-photo')
                            img = await img_elem.get_attribute('src') if img_elem else ""
                            
                            # Fiyat
                            price_elem = await product.query_selector('.price')
                            price = await price_elem.inner_text() if price_elem else "N/A"
                            
                            self.products.append({
                                'name': name.strip(),
                                'url': link,
                                'image': img,
                                'price': price.strip(),
                                'page': page_num
                            })
                            
                            print(f"      âœ“ {name.strip()}")
                            
                        except Exception as e:
                            print(f"      âš ï¸ ÃœrÃ¼n parse hatasÄ±: {e}")
                            continue
                    
                    page_num += 1
                    await asyncio.sleep(2)  # Rate limiting
                    
                except Exception as e:
                    print(f"âŒ Sayfa hatasÄ±: {e}")
                    break
            
            await browser.close()
            
        return self.products
    
    async def scrape_product_details(self, product_url: str):
        """Tekil Ã¼rÃ¼n sayfasÄ±ndan detaylÄ± hikaye Ã§eker"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = await context.new_page()
            
            try:
                await page.goto(product_url, wait_until='networkidle', timeout=30000)
                
                # ÃœrÃ¼n aÃ§Ä±klamasÄ±/hikayesi
                story_selectors = [
                    '.product-description',
                    '.product-info-main',
                    '[itemprop="description"]',
                    '.product-attribute'
                ]
                
                story = ""
                for selector in story_selectors:
                    elem = await page.query_selector(selector)
                    if elem:
                        story = await elem.inner_text()
                        break
                
                await browser.close()
                return story.strip() if story else "Hikaye bulunamadÄ±"
                
            except Exception as e:
                await browser.close()
                return f"Hata: {e}"
    
    def save_to_csv(self, filename='pasabahce_products.csv'):
        """ÃœrÃ¼nleri CSV'ye kaydet"""
        if not self.products:
            print("âš ï¸ Kaydedilecek Ã¼rÃ¼n yok!")
            return
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['name', 'url', 'image', 'price', 'page'])
            writer.writeheader()
            writer.writerows(self.products)
        
        print(f"\nğŸ’¾ {len(self.products)} Ã¼rÃ¼n '{filename}' dosyasÄ±na kaydedildi!")
    
    def save_to_json(self, filename='pasabahce_products.json'):
        """ÃœrÃ¼nleri JSON'a kaydet"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.products, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ {len(self.products)} Ã¼rÃ¼n '{filename}' dosyasÄ±na kaydedildi!")


# KullanÄ±m
async def main():
    scraper = PasabahceScraper()
    
    # TÃ¼m Ã¼rÃ¼nleri tara
    products = await scraper.scrape_all_products()
    
    # Kaydet
    scraper.save_to_csv()
    scraper.save_to_json()
    
    # Ä°steÄŸe baÄŸlÄ±: Ä°lk 5 Ã¼rÃ¼nÃ¼n detayÄ±nÄ± Ã§ek
    print("\nğŸ” Ä°lk 5 Ã¼rÃ¼nÃ¼n detaylarÄ± Ã§ekiliyor...")
    for product in products[:5]:
        if product['url']:
            story = await scraper.scrape_product_details(product['url'])
            product['story'] = story
            print(f"\nğŸ“– {product['name']}")
            print(f"   {story[:200]}...")
    
    # GÃ¼ncellenmiÅŸ veriyi kaydet
    scraper.save_to_json('pasabahce_detailed.json')

if __name__ == "__main__":
    # Playwright kurulumu iÃ§in:
    # pip install playwright
    # playwright install chromium
    
    asyncio.run(main())
